{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc00555",
   "metadata": {},
   "source": [
    "# Test out data augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentCIFAR import augmentCIFAR\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Load, partition, and resize CIFAR10 Data ##\n",
    "##############################################\n",
    "def loadData():\n",
    "    import pickle\n",
    "\n",
    "    # unpickle the binary files\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    labels = ['airplane',  # index 0\n",
    "          'automobile',  # index 1\n",
    "          'bird',  # index 2 \n",
    "          'cat',  # index 3 \n",
    "          'deer',  # index 4\n",
    "          'dog',  # index 5\n",
    "          'frog',  # index 6 \n",
    "          'horse',  # index 7 \n",
    "          'ship',  # index 8 \n",
    "          'truck']  # index 9\n",
    "    \n",
    "    # paths to each batch of data\n",
    "    batch1 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_5\")\n",
    "    meta = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/batches.meta\")\n",
    "    test = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "    # separate labels and image data from each batch\n",
    "    y_train1 = batch1[b'labels']\n",
    "    x_train1 = batch1[b'data']\n",
    "    y_train2 = batch2[b'labels']\n",
    "    x_train2 = batch2[b'data']\n",
    "    y_train3 = batch3[b'labels']\n",
    "    x_train3 = batch3[b'data']\n",
    "    y_train4 = batch4[b'labels']\n",
    "    x_train4 = batch4[b'data']\n",
    "    y_train5 = batch5[b'labels']\n",
    "    x_train5 = batch5[b'data']\n",
    "\n",
    "    # concatenate into big training and testing arrays\n",
    "    y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5))\n",
    "    x_train = np.concatenate((x_train1, x_train2, x_train3, x_train4, x_train5), axis=0)\n",
    "    \n",
    "    y_test = test[b'labels']\n",
    "    x_test = test[b'data']\n",
    "    \n",
    "    # Further break training data into train / validation sets \n",
    "    # put 5000 into validation set and keep remaining 45,000 for train\n",
    "    (x_train, x_valid) = x_train[1000:], x_train[:1000] \n",
    "    (y_train, y_valid) = y_train[1000:], y_train[:1000]\n",
    "\n",
    "    # reshape data to match dimensions of cifar10.load_data\n",
    "    x_train = x_train.reshape(49000, 3, 32, 32)\n",
    "    x_train = x_train.transpose(0, 2, 3, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_train /= 255\n",
    "\n",
    "    x_valid = x_valid.reshape(1000, 3, 32, 32)\n",
    "    x_valid = x_valid.transpose(0, 2, 3, 1)\n",
    "    x_valid = x_valid.astype('float32')\n",
    "    x_valid /= 255\n",
    "\n",
    "    x_test = x_test.reshape(10000, 3, 32, 32)\n",
    "    x_test = x_test.transpose(0, 2, 3, 1)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /= 255\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aef8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dfd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_x_train, augmented_y_train = augmentCIFAR(x_train, y_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(augmented_x_train.shape)\n",
    "print(augmented_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8122746",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_og_imgs = x_train.shape[0]\n",
    "fig, ax = plt.subplots(10, 2, figsize=(8, 40))\n",
    "\n",
    "for i in range(10):\n",
    "    rand_idx = random.randint(0, num_og_imgs)\n",
    "   \n",
    "    # print original image\n",
    "    ax[i, 0].imshow(augmented_x_train[rand_idx])\n",
    "    ax[i, 0].set_title(\"Image Idx: \" + str(rand_idx) + \", original\")\n",
    "    ax[i, 0].set_yticks([])\n",
    "    ax[i, 0].set_xticks([])\n",
    "\n",
    "\n",
    "    # print augmented image\n",
    "    ax[i, 1].imshow(augmented_x_train[rand_idx + num_og_imgs])\n",
    "    ax[i, 1].set_title(\"Image Idx: \" + str(rand_idx + num_og_imgs) + \", augmented\")\n",
    "    ax[i, 1].set_yticks([])\n",
    "    ax[i, 1].set_xticks([])\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770bebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9affdd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
