{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e90bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from simple_contrastive_loss import contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0199eda",
   "metadata": {},
   "source": [
    "# Load CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c6c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Load, partition, and resize CIFAR10 Data ##\n",
    "##############################################\n",
    "def loadData():\n",
    "    import pickle\n",
    "\n",
    "    # unpickle the binary files\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    labels = ['airplane',  # index 0\n",
    "          'automobile',  # index 1\n",
    "          'bird',  # index 2 \n",
    "          'cat',  # index 3 \n",
    "          'deer',  # index 4\n",
    "          'dog',  # index 5\n",
    "          'frog',  # index 6 \n",
    "          'horse',  # index 7 \n",
    "          'ship',  # index 8 \n",
    "          'truck']  # index 9\n",
    "    \n",
    "    # paths to each batch of data\n",
    "    batch1 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_5\")\n",
    "    meta = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/batches.meta\")\n",
    "    test = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "    # separate labels and image data from each batch\n",
    "    y_train1 = batch1[b'labels']\n",
    "    x_train1 = batch1[b'data']\n",
    "    y_train2 = batch2[b'labels']\n",
    "    x_train2 = batch2[b'data']\n",
    "    y_train3 = batch3[b'labels']\n",
    "    x_train3 = batch3[b'data']\n",
    "    y_train4 = batch4[b'labels']\n",
    "    x_train4 = batch4[b'data']\n",
    "    y_train5 = batch5[b'labels']\n",
    "    x_train5 = batch5[b'data']\n",
    "\n",
    "    # concatenate into big training and testing arrays\n",
    "    y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5))\n",
    "    x_train = np.concatenate((x_train1, x_train2, x_train3, x_train4, x_train5), axis=0)\n",
    "    \n",
    "    y_test = test[b'labels']\n",
    "    x_test = test[b'data']\n",
    "    \n",
    "    # Further break training data into train / validation sets \n",
    "    # put 5000 into validation set and keep remaining 45,000 for train\n",
    "    (x_train, x_valid) = x_train[1000:], x_train[:1000] \n",
    "    (y_train, y_valid) = y_train[1000:], y_train[:1000]\n",
    "\n",
    "    # reshape data to match dimensions of cifar10.load_data\n",
    "    x_train = x_train.reshape(49000, 3, 32, 32)\n",
    "    x_train = x_train.transpose(0, 2, 3, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_train /= 255\n",
    "\n",
    "    x_valid = x_valid.reshape(1000, 3, 32, 32)\n",
    "    x_valid = x_valid.transpose(0, 2, 3, 1)\n",
    "    x_valid = x_valid.astype('float32')\n",
    "    x_valid /= 255\n",
    "\n",
    "    x_test = x_test.reshape(10000, 3, 32, 32)\n",
    "    x_test = x_test.transpose(0, 2, 3, 1)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /= 255\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "#     y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "#     y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "#     y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    \n",
    "    # preprocess data to convert from RGB -> BGR and to zero center around ImageNet dataset\n",
    "    x_train = tf.keras.applications.resnet50.preprocess_input(x_train)\n",
    "    x_valid = tf.keras.applications.resnet50.preprocess_input(x_valid)\n",
    "    x_test = tf.keras.applications.resnet50.preprocess_input(x_test)\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc75cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb41b4",
   "metadata": {},
   "source": [
    "# Build Model with Custom Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47605188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "## Load weights for ResNet50 & add classifier head ##\n",
    "#####################################################\n",
    "def build_base_model(sims, bsz):\n",
    "    conv_base = tf.keras.models.load_model(\"~/scratch/gpfs/eysu/SoftCL/models/ResNet50_weights\")\n",
    "    \n",
    "    # add classifier on top of conv_base\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # upsample to resize inputs of CIFAR10 from (32x32x3) to (256x256x3)\n",
    "    model.add(layers.UpSampling2D(size=(2,2)))\n",
    "    model.add(layers.UpSampling2D(size=(2,2)))\n",
    "    model.add(layers.UpSampling2D(size=(2,2)))\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    embed_layer = model.layers[-4]\n",
    "    \n",
    "    loss_params = (sims, embed_layer, bsz)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=contrastive_loss(loss_params), metrics=['accuracy'], run_eagerly=True)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27ab21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title):\n",
    "    \n",
    "    plt.plot(history.history[metric_name],color='blue',label='training_' + metric_name)\n",
    "    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)\n",
    "    \n",
    "    if metric_name == 'loss':\n",
    "        plt.ylim([0,200])\n",
    "\n",
    "    elif metric_name == 'accuracy':\n",
    "        plt.ylim([0,1])\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ea399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "plot_metrics(\"loss\", \"Loss Curve\")\n",
    "plot_metrics(\"accuracy\", \"Accuracy Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5b809",
   "metadata": {},
   "source": [
    "# Use SimCLR similarity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simCLR_CIFAR import findSims\n",
    "# load data\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()\n",
    "\n",
    "# generate similarity matrix\n",
    "sims = findSims(x_test, y_test)\n",
    "\n",
    "# build model\n",
    "model = build_base_model(sims, 64)\n",
    "\n",
    "# expand labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_test, y_test, epochs=100, batch_size=64, validation_data = (x_valid, y_valid))\n",
    "\n",
    "# plot metrics\n",
    "plot_metrics(\"loss\", \"Loss Curve\")\n",
    "plot_metrics(\"accuracy\", \"Accuracy Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807d016",
   "metadata": {},
   "source": [
    "# Use SupCon Similarity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supCon_CIFAR import findSims\n",
    "# load data\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()\n",
    "\n",
    "# generate similarity matrix\n",
    "sims = findSims(x_test, y_test)\n",
    "\n",
    "# build model\n",
    "model = build_base_model(sims, 64)\n",
    "\n",
    "# expand labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_test, y_test, epochs=100, batch_size=64, validation_data = (x_valid, y_valid))\n",
    "\n",
    "# plot metrics\n",
    "plot_metrics(\"loss\", \"Loss Curve\")\n",
    "plot_metrics(\"accuracy\", \"Accuracy Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662b893",
   "metadata": {},
   "source": [
    "# Use SoftCL Similarity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee880fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softCLSims import findSims\n",
    "# load data\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()\n",
    "\n",
    "# generate similarity matrix\n",
    "soft_sims = np.load('/scratch/gpfs/eysu/src_data/cifar-10h/data/pairwise_sims.npy')\n",
    "sims = findSims(x_test, y_test, soft_sims)\n",
    "\n",
    "# build model\n",
    "model = build_base_model(sims, 64)\n",
    "\n",
    "# expand labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_test, y_test, epochs=100, batch_size=64, validation_data = (x_valid, y_valid))\n",
    "\n",
    "# plot metrics\n",
    "plot_metrics(\"loss\", \"Loss Curve\")\n",
    "plot_metrics(\"accuracy\", \"Accuracy Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640529b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
