{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b459ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702515b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Load, partition, and resize CIFAR10 Data ##\n",
    "##############################################\n",
    "def loadData():\n",
    "    import pickle\n",
    "\n",
    "    # unpickle the binary files\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    labels = ['airplane',  # index 0\n",
    "          'automobile',  # index 1\n",
    "          'bird',  # index 2 \n",
    "          'cat',  # index 3 \n",
    "          'deer',  # index 4\n",
    "          'dog',  # index 5\n",
    "          'frog',  # index 6 \n",
    "          'horse',  # index 7 \n",
    "          'ship',  # index 8 \n",
    "          'truck']  # index 9\n",
    "    \n",
    "    # paths to each batch of data\n",
    "    batch1 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/data_batch_5\")\n",
    "    meta = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/batches.meta\")\n",
    "    test = unpickle(\"/scratch/gpfs/eysu/src_data/cifar-10-batches-py/test_batch\")\n",
    "\n",
    "    # separate labels and image data from each batch\n",
    "    y_train1 = batch1[b'labels']\n",
    "    x_train1 = batch1[b'data']\n",
    "    y_train2 = batch2[b'labels']\n",
    "    x_train2 = batch2[b'data']\n",
    "    y_train3 = batch3[b'labels']\n",
    "    x_train3 = batch3[b'data']\n",
    "    y_train4 = batch4[b'labels']\n",
    "    x_train4 = batch4[b'data']\n",
    "    y_train5 = batch5[b'labels']\n",
    "    x_train5 = batch5[b'data']\n",
    "\n",
    "    # concatenate into big training and testing arrays\n",
    "    y_train = np.concatenate((y_train1, y_train2, y_train3, y_train4, y_train5))\n",
    "    x_train = np.concatenate((x_train1, x_train2, x_train3, x_train4, x_train5), axis=0)\n",
    "    \n",
    "    y_test = test[b'labels']\n",
    "    x_test = test[b'data']\n",
    "    \n",
    "    # Further break training data into train / validation sets \n",
    "    # put 5000 into validation set and keep remaining 45,000 for train\n",
    "    (x_train, x_valid) = x_train[1000:], x_train[:1000] \n",
    "    (y_train, y_valid) = y_train[1000:], y_train[:1000]\n",
    "\n",
    "    # reshape data to match dimensions of cifar10.load_data\n",
    "    x_train = x_train.reshape(49000, 3, 32, 32)\n",
    "    x_train = x_train.transpose(0, 2, 3, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_train /= 255\n",
    "\n",
    "    x_valid = x_valid.reshape(1000, 3, 32, 32)\n",
    "    x_valid = x_valid.transpose(0, 2, 3, 1)\n",
    "    x_valid = x_valid.astype('float32')\n",
    "    x_valid /= 255\n",
    "\n",
    "    x_test = x_test.reshape(10000, 3, 32, 32)\n",
    "    x_test = x_test.transpose(0, 2, 3, 1)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /= 255\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "#     y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "#     y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "#     y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test, labels\n",
    "\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test, labels = loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e615128",
   "metadata": {},
   "source": [
    "# Generate SimCLR Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebacc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simCLR_CIFAR import findSims\n",
    "\n",
    "sims = findSims(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2feb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sims, cmap='hot', interpolation='nearest')\n",
    "plt.title(\"Similarity Matrix for CIFAR10 Test Images using SimCLR method\")\n",
    "plt.xlabel(\"Image Indices\")\n",
    "plt.ylabel(\"Image Indices\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2a335",
   "metadata": {},
   "source": [
    "# Generate SupCon Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68482b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supCon_CIFAR import findSims\n",
    "\n",
    "sims = findSims(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4affe474",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sims, cmap='hot', interpolation='nearest')\n",
    "plt.title(\"Similarity Matrix for CIFAR10 Test Images using SupCon method\")\n",
    "plt.xlabel(\"Image Indices\")\n",
    "plt.ylabel(\"Image Indices\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c87b3",
   "metadata": {},
   "source": [
    "# Generate SoftCL Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softCLSims import findSims\n",
    "\n",
    "# load in CIFAR10H similarity data\n",
    "soft = np.load('/scratch/gpfs/eysu/src_data/cifar-10h/data/cifar10h-probs.npy')\n",
    "\n",
    "sims = findSims(x_test, y_test, soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b06e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sims, cmap='hot', interpolation='nearest')\n",
    "plt.title(\"Similarity Matrix for CIFAR10 Test Images using SoftCL method\")\n",
    "plt.xlabel(\"Image Indices\")\n",
    "plt.ylabel(\"Image Indices\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ea973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
